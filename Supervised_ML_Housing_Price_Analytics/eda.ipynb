{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d4f18bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor as rfr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8c1a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb457b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42f7e540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1460, 81)\n",
      "Test shape: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Data/train.csv\")\n",
    "test_df = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf321310",
   "metadata": {},
   "source": [
    "### Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "385c9888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (2919, 79)\n"
     ]
    }
   ],
   "source": [
    "train_ID = train_df['Id']\n",
    "test_ID = test_df['Id']\n",
    "\n",
    "y_train = train_df['SalePrice']\n",
    "\n",
    "# drop ID and target\n",
    "train_df.drop(['Id','SalePrice'], axis=1, inplace=True)\n",
    "test_df.drop(['Id'], axis=1, inplace=True)\n",
    "\n",
    "all_data = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "print(\"Combined data shape:\", all_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e95696",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c876ff",
   "metadata": {},
   "source": [
    "##### 1. Missing Values Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fca3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill categorical missing with 'None'\n",
    "for col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu',\n",
    "            'GarageType','GarageFinish','GarageQual','GarageCond',\n",
    "            'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']:\n",
    "    all_data[col] = all_data[col].fillna('None')\n",
    "\n",
    "# Fill numerical missing with 0\n",
    "for col in ['GarageYrBlt','GarageArea','GarageCars',\n",
    "            'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n",
    "            'BsmtFullBath','BsmtHalfBath','MasVnrArea']:\n",
    "    all_data[col] = all_data[col].fillna(0)\n",
    "\n",
    "# Fill mode\n",
    "all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n",
    "\n",
    "# LotFrontage â†’ fill by median of neighborhood\n",
    "all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38b97e",
   "metadata": {},
   "source": [
    "##### 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c599138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total square footage\n",
    "all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n",
    "\n",
    "# Total bathrooms\n",
    "all_data['TotalBath'] = (all_data['FullBath'] + 0.5*all_data['HalfBath'] +\n",
    "                         all_data['BsmtFullBath'] + 0.5*all_data['BsmtHalfBath'])\n",
    "\n",
    "# Total porch area\n",
    "all_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['EnclosedPorch'] +\n",
    "                            all_data['3SsnPorch'] + all_data['ScreenPorch'])\n",
    "\n",
    "# Convert some categorical ordinals to numbers\n",
    "quality_map = {'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\n",
    "for col in ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual',\n",
    "            'FireplaceQu','GarageQual','GarageCond']:\n",
    "    all_data[col] = all_data[col].map(quality_map).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14507ae",
   "metadata": {},
   "source": [
    "##### 3. Handling Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c56c80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: x.dropna().skew()).sort_values(ascending=False)\n",
    "skewed = skewed_feats[skewed_feats > 0.75].index\n",
    "\n",
    "all_data[skewed] = np.log1p(all_data[skewed])\n",
    "\n",
    "# Log-transform target variable too\n",
    "y_train = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ece390",
   "metadata": {},
   "source": [
    "##### 4. Encoding Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b52451c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final all_data shape: (2919, 268)\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.get_dummies(all_data)\n",
    "\n",
    "print(\"Final all_data shape:\", all_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9bacf",
   "metadata": {},
   "source": [
    "### Model Creation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "114294e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1460, 268) X_test: (1459, 268)\n"
     ]
    }
   ],
   "source": [
    "# Split Data into Train and Test\n",
    "\n",
    "X_train = all_data[:train_df.shape[0]]\n",
    "X_test_final = all_data[train_df.shape[0]:]\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"X_test:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "84e8b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "\n",
    "models = {\n",
    "    \"Lasso\": Lasso(alpha=0.0005, random_state=42, max_iter=10000),\n",
    "    \"Ridge\": Ridge(alpha=10),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42, max_iter=10000),\n",
    "    \"RandomForest\": rfr(n_estimators=300, random_state=42),\n",
    "    \"GradientBoosting\": gbr(n_estimators=3000, learning_rate=0.05,\n",
    "                                                 max_depth=4, max_features='sqrt', \n",
    "                                                 min_samples_leaf=15, min_samples_split=10,\n",
    "                                                 random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                max_depth=4, subsample=0.7,\n",
    "                                colsample_bytree=0.7, random_state=42),\n",
    "    \"LightGBM\": lgb.LGBMRegressor(objective='regression', num_leaves=5,\n",
    "                                  learning_rate=0.05, n_estimators=3000,\n",
    "                                  max_bin=200, bagging_fraction=0.8,\n",
    "                                  bagging_freq=5, feature_fraction=0.8,\n",
    "                                  random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21013fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: 0.12598\n",
      "Ridge: 0.13066\n",
      "ElasticNet: 0.12599\n",
      "RandomForest: 0.14183\n",
      "GradientBoosting: 0.12993\n",
      "XGBoost: 0.12503\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 171\n",
      "[LightGBM] [Info] Start training from score 12.030658\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3293\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 175\n",
      "[LightGBM] [Info] Start training from score 12.016898\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3279\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 171\n",
      "[LightGBM] [Info] Start training from score 12.022759\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000376 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3296\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 12.027933\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3294\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 170\n",
      "[LightGBM] [Info] Start training from score 12.022040\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "LightGBM: 0.13109\n"
     ]
    }
   ],
   "source": [
    "# Cross validation and evaluation\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmse_cv(model, X, y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return rmse.mean()\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    score = rmse_cv(model, X_train, y_train)\n",
    "    results[name] = score\n",
    "    print(f\"{name}: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "88ebf10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: XGBoost with RMSE = 0.12503\n"
     ]
    }
   ],
   "source": [
    "# Picking the best model based on RMSE\n",
    "\n",
    "best_model_name = min(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} with RMSE = {results[best_model_name]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… submission.csv saved with best model predictions!\n"
     ]
    }
   ],
   "source": [
    "# Train best model on full data\n",
    "best_model.fit(X_train, y_train)\n",
    "preds = best_model.predict(X_test_final)\n",
    "final_preds = np.expm1(preds)\n",
    "\n",
    "submission = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": final_preds})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"âœ… submission.csv saved with best model predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9394c0",
   "metadata": {},
   "source": [
    "### Ensemble Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdc9fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: 0.12598\n",
      "Ridge: 0.13066\n",
      "ElasticNet: 0.12599\n",
      "RandomForest: 0.14183\n",
      "GradientBoosting: 0.12993\n",
      "XGBoost: 0.12503\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000391 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3295\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 171\n",
      "[LightGBM] [Info] Start training from score 12.030658\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3293\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 175\n",
      "[LightGBM] [Info] Start training from score 12.016898\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000764 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3279\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 171\n",
      "[LightGBM] [Info] Start training from score 12.022759\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3296\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 173\n",
      "[LightGBM] [Info] Start training from score 12.027933\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3294\n",
      "[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 170\n",
      "[LightGBM] [Info] Start training from score 12.022040\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "LightGBM: 0.13109\n",
      "\n",
      "Model Ranking (by RMSE):\n",
      "XGBoost: 0.12503\n",
      "Lasso: 0.12598\n",
      "ElasticNet: 0.12599\n",
      "GradientBoosting: 0.12993\n",
      "Ridge: 0.13066\n",
      "LightGBM: 0.13109\n",
      "RandomForest: 0.14183\n",
      "\n",
      "Top 3 Models: ['XGBoost', 'Lasso', 'ElasticNet']\n",
      "âœ… Saved: submission_avg.csv and submission_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Train all models individually\n",
    "# ===============================\n",
    "for name, model in models.items():\n",
    "    score = rmse_cv(model, X_train, y_train)\n",
    "    results[name] = score\n",
    "    print(f\"{name}: {score:.5f}\")\n",
    "\n",
    "# Sort models by CV score\n",
    "sorted_results = dict(sorted(results.items(), key=lambda x: x[1]))\n",
    "print(\"\\nModel Ranking (by RMSE):\")\n",
    "for k, v in sorted_results.items():\n",
    "    print(f\"{k}: {v:.5f}\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Train Top 3 Models Fully\n",
    "# ===============================\n",
    "top3_names = list(sorted_results.keys())[:3]\n",
    "print(f\"\\nTop 3 Models: {top3_names}\")\n",
    "\n",
    "trained_models = {}\n",
    "for name in top3_names:\n",
    "    model = models[name]\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "\n",
    "# ===============================\n",
    "# 3. Make Predictions\n",
    "# ===============================\n",
    "predictions = []\n",
    "for name, model in trained_models.items():\n",
    "    preds = model.predict(X_test_final)\n",
    "    preds = np.expm1(preds)  # inverse log-transform\n",
    "    predictions.append(preds)\n",
    "\n",
    "# ===============================\n",
    "# 4. Blend Predictions\n",
    "# ===============================\n",
    "# Simple average ensemble\n",
    "final_preds = (predictions[0] + predictions[1] + predictions[2]) / 3\n",
    "\n",
    "# Weighted average (give higher weight to best model)\n",
    "weights = [0.5, 0.3, 0.2]   # adjust if needed\n",
    "weighted_preds = (weights[0]*predictions[0] +\n",
    "                  weights[1]*predictions[1] +\n",
    "                  weights[2]*predictions[2])\n",
    "\n",
    "# ===============================\n",
    "# 5. Save Submissions\n",
    "# ===============================\n",
    "submission_avg = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": final_preds})\n",
    "submission_avg.to_csv(\"submission_avg.csv\", index=False)\n",
    "\n",
    "submission_weighted = pd.DataFrame({\"Id\": test_ID, \"SalePrice\": weighted_preds})\n",
    "submission_weighted.to_csv(\"submission_weighted.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Saved: submission_avg.csv and submission_weighted.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
